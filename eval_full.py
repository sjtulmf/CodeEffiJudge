#!/usr/bin/env python3
"""
LLM ä»£ç æ€§èƒ½åˆ¤æ–­è¯„ä¼° v9 - ç®€åŒ–ä¸º3ç§Promptç­–ç•¥

æ”¯æŒ 3 ç§ Prompt ç­–ç•¥ï¼Œæ‰€æœ‰ç¤ºä¾‹å‡ä¸ºè¯­è¨€ç‰¹å®š:

| Promptç­–ç•¥          | ç¤ºä¾‹   | CoTæ¨ç† | è¯­è¨€ç‰¹å®šç¤ºä¾‹ |
|---------------------|--------|---------|--------------|  
| zero-shot (ZS)      |   âŒ   |   âŒ    |      -       |
| few-shot (FS)       |   âœ…   |   âŒ    |   âœ…(åŒ¹é…)   |
| few-shot-cot (FS-CoT)| âœ…   |   âœ…    |   âœ…(åŒ¹é…)   |

åªæ”¯æŒæˆå¯¹æ¯”è¾ƒ (pair-wise comparison):
- åŒä»£ç å¯¹æ¯”ï¼Œè¾“å‡º A æˆ– B (è¡¨ç¤ºå“ªä¸ªä»£ç æ›´å¿«)
"""

import json
import time
import re
import argparse
import os
from pathlib import Path
from typing import Optional, Dict, List, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from dataclasses import dataclass, field, asdict
from openai import OpenAI

# ==================== é…ç½® ====================
API_KEY = ""
BASE_URL = ""
MODEL = ""

# ==================== æ¨¡å‹å®šä»·è¡¨ (USD per 1M tokens) ====================
MODEL_PRICING = {
    # DeepSeek ç³»åˆ—
    "deepseek-v3": {"input": 0.27, "output": 1.10},
    "deepseek-v3.2": {"input": 0.27, "output": 1.10},
    "deepseek-chat": {"input": 0.14, "output": 0.28},
    "deepseek-coder": {"input": 0.14, "output": 0.28},
    
    # OpenAI GPT ç³»åˆ—
    "gpt-4o": {"input": 2.50, "output": 10.00},
    "gpt-4o-mini": {"input": 0.150, "output": 0.600},
    "gpt-4-turbo": {"input": 10.00, "output": 30.00},
    "gpt-3.5-turbo": {"input": 0.50, "output": 1.50},
    "gpt-5-mini": {"input": 0.40, "output": 1.60},
    "gpt-5.2": {"input": 5.00, "output": 15.00},
    
    # Anthropic Claude ç³»åˆ—
    "claude-3-opus": {"input": 15.00, "output": 75.00},
    "claude-3-sonnet": {"input": 3.00, "output": 15.00},
    "claude-3-haiku": {"input": 0.25, "output": 1.25},
    "claude-sonnet-4": {"input": 3.00, "output": 15.00},
    "claude-sonnet-4-5": {"input": 3.00, "output": 15.00},
    
    # Qwen ç³»åˆ—
    "qwen-turbo": {"input": 0.30, "output": 0.60},
    "qwen-plus": {"input": 0.80, "output": 2.00},
    "qwen-max": {"input": 2.00, "output": 6.00},
    "qwen3-235b": {"input": 2.00, "output": 6.00},
    "qwen3-30b": {"input": 0.50, "output": 1.50},
    "qwen3-coder-480b": {"input": 2.50, "output": 7.50},
    "qwen3-coder-30b": {"input": 0.50, "output": 1.50},
    
    # é»˜è®¤å®šä»·ï¼ˆå¦‚æœæ¨¡å‹æœªåœ¨åˆ—è¡¨ä¸­ï¼‰
    "default": {"input": 5, "output": 15}
}


def get_model_pricing(model_name: str) -> dict:
    """
    è·å–æ¨¡å‹å®šä»·ä¿¡æ¯
    
    Args:
        model_name: æ¨¡å‹åç§°
    
    Returns:
        åŒ…å« input å’Œ output ä»·æ ¼çš„å­—å…¸ (USD per 1M tokens)
    """
    model_lower = model_name.lower()
    
    # ç²¾ç¡®åŒ¹é…
    for key, pricing in MODEL_PRICING.items():
        if key == "default":
            continue
        if key in model_lower:
            return pricing
    
    # ä½¿ç”¨é»˜è®¤å®šä»·
    print(f"[WARN] æ¨¡å‹ '{model_name}' æœªåœ¨å®šä»·è¡¨ä¸­ï¼Œä½¿ç”¨é»˜è®¤å®šä»·")
    return MODEL_PRICING["default"]


def calculate_cost(prompt_tokens: int, completion_tokens: int, model_name: str) -> float:
    """
    è®¡ç®—APIè°ƒç”¨æˆæœ¬
    
    Args:
        prompt_tokens: è¾“å…¥tokenæ•°
        completion_tokens: è¾“å‡ºtokenæ•°
        model_name: æ¨¡å‹åç§°
    
    Returns:
        æˆæœ¬ï¼ˆç¾å…ƒï¼‰
    """
    pricing = get_model_pricing(model_name)
    
    # ä»·æ ¼æ˜¯ per 1M tokensï¼Œæ‰€ä»¥éœ€è¦é™¤ä»¥ 1,000,000
    input_cost = (prompt_tokens / 1_000_000) * pricing["input"]
    output_cost = (completion_tokens / 1_000_000) * pricing["output"]
    
    return input_cost + output_cost


# ==================== è¾…åŠ©å‡½æ•°ï¼šæ¨¡å‹åç§°å¤„ç† ====================
def sanitize_model_name_for_filename(model_name: str) -> str:
    """
    å°†æ¨¡å‹åç§°æ¸…ç†ä¸ºé€‚åˆä½œä¸ºæ–‡ä»¶åçš„å­—ç¬¦ä¸²
    
    Args:
        model_name: åŸå§‹æ¨¡å‹åç§°ï¼Œå¦‚ "deepseek-v3.2", "gpt-4-turbo", "claude-3-opus"
    
    Returns:
        æ¸…ç†åçš„å­—ç¬¦ä¸²ï¼Œåªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦
    
    Examples:
        "deepseek-v3.2" -> "deepseek-v3.2"
        "gpt-4-turbo-2024-04-09" -> "gpt-4-turbo-2024-04-09"
        "claude/3/opus" -> "claude_3_opus"
        "model:latest" -> "model_latest"
    """
    if not model_name:
        return ""
    
    import re as re_module
    
    # å°†è·¯å¾„åˆ†éš”ç¬¦å’Œå†’å·æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    cleaned = model_name.replace('/', '_').replace('\\', '_').replace(':', '_')
    
    # åªä¿ç•™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦å’Œç‚¹
    cleaned = re_module.sub(r'[^a-zA-Z0-9_\-.]', '_', cleaned)
    
    # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
    cleaned = re_module.sub(r'_+', '_', cleaned)
    
    # ç§»é™¤é¦–å°¾çš„ä¸‹åˆ’çº¿å’Œç‚¹
    cleaned = cleaned.strip('_.')
    
    return cleaned


def get_model_suffix(model_name: str) -> str:
    """
    è·å–ç”¨äºæ–‡ä»¶åçš„æ¨¡å‹åç¼€
    
    Args:
        model_name: æ¨¡å‹åç§°
    
    Returns:
        å¸¦ä¸‹åˆ’çº¿å‰ç¼€çš„æ¨¡å‹åç¼€ï¼Œå¦‚ "_deepseek-v3.2"ï¼Œè‹¥æ¨¡å‹åä¸ºç©ºåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    cleaned = sanitize_model_name_for_filename(model_name)
    if cleaned:
        return f"_{cleaned}"
    return ""


def generate_default_output_filename(input_path: str, prompt_type: str, model_name: str = None) -> str:
    """
    æ ¹æ®è¾“å…¥æ–‡ä»¶è·¯å¾„ã€promptç±»å‹å’Œæ¨¡å‹åç§°ç”Ÿæˆé»˜è®¤è¾“å‡ºæ–‡ä»¶å
    
    Args:
        input_path: è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„
        prompt_type: promptç±»å‹ (zero-shot, few-shot)
        model_name: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œæ ¼å¼ä¸º: {input_stem}_results_{prompt_type}_{model}.json
        ä¾‹å¦‚: cpp_natural_seed42_sanitized_results_zero-shot_deepseek-v3.2.json
    """
    input_p = Path(input_path)
    model_suffix = get_model_suffix(model_name) if model_name else ""
    output_name = f"{input_p.stem}_results_{prompt_type}{model_suffix}.json"
    return str(input_p.parent / output_name)


# ==================== æ•°æ®é›†ç±»å‹æ£€æµ‹ ====================
def detect_dataset_type(data: dict) -> str:
    """æ£€æµ‹æ•°æ®é›†ç±»å‹"""
    scheme = data.get("scheme", "")
    if scheme:
        return scheme
    
    if "medium_code" in data or "medium_time" in data:
        return "triple"
    elif "raw_code" in data and "clean_code" in data:
        return "clean_vs_raw"
    elif "tier" in data and data.get("tier") in ["fast", "medium", "slow"]:
        return "same_tier"
    elif "pair_type" in data:
        return "cross_tier"
    elif "slow_code" in data and "fast_code" in data:
        return "cross_tier"
    elif "src_code" in data and "tgt_code" in data:
        return "effibench_pair"
    
    return "unknown"


def get_expected_answer_type(data: dict) -> str:
    """è·å–æœŸæœ›çš„ç­”æ¡ˆç±»å‹"""
    expected = data.get("expected_answer", "")
    if expected:
        if expected.lower() == "similar":
            return "similar"
        else:
            return "fast"
    
    dataset_type = detect_dataset_type(data)
    if dataset_type in ("same_tier", "clean_vs_raw"):
        return "similar"
    else:
        return "fast"


# ==================== æ”¹è¿›çš„ Prompt æ¨¡æ¿ ====================
class PromptTemplates:
    """æ”¹è¿›çš„Promptæ¨¡æ¿ - v8 å®Œæ•´å®éªŒè®¾è®¡"""
    
    @staticmethod
    def get_lang_display(lang: str) -> str:
        if lang in ("python", "py", "python3"):
            return "Python"
        elif lang in ("cpp", "c++", "cc"):
            return "C++"
        elif lang in ("java",):
            return "Java"
        return lang.upper()
    
    # ==================== Few-Shot ç¤ºä¾‹åº“ ====================
    
    @staticmethod
    def get_examples_for_lang(lang: str, with_cot: bool = True) -> str:
        """è·å–è¯­è¨€ç‰¹å®šçš„Few-Shotç¤ºä¾‹"""
        lang_lower = lang.lower()
        
        if lang_lower in ("python", "py", "python3"):
            return PromptTemplates._get_python_examples(with_cot)
        elif lang_lower in ("cpp", "c++", "cc"):
            return PromptTemplates._get_cpp_examples(with_cot)
        elif lang_lower in ("java",):
            return PromptTemplates._get_java_examples(with_cot)
        else:
            # é»˜è®¤ä½¿ç”¨C++ç¤ºä¾‹
            return PromptTemplates._get_cpp_examples(with_cot)
    
    @staticmethod
    def _get_cpp_examples(with_cot: bool = True) -> str:
        """C++ Few-Shot ç¤ºä¾‹"""
        if with_cot:
            return """
### Example 1:

Code A:
```cpp
int sum = 0;
for (int i = 0; i < n; i++) {
    sum += arr[i];
}
```

Code B:
```cpp
int sum = 0;
for (int i = 0; i < n; i++) {
    for (int j = 0; j <= i; j++) {
        if (j == i) sum += arr[i];
    }
}
```

**Code A Analysis:**
- Algorithm/Approach: Direct summation
- Time Complexity: O(n)
- Key Operations: n additions

**Code B Analysis:**
- Algorithm/Approach: Unnecessary nested loop
- Time Complexity: O(nÂ²)
- Key Operations: nÂ²/2 comparisons + n additions

**Answer: A**

### Example 2:

Code A:
```cpp
bool found = false;
for (int i = 0; i < n; i++) {
    if (arr[i] == target) found = true;
}
return found;
```

Code B:
```cpp
for (int i = 0; i < n; i++) {
    if (arr[i] == target) return true;
}
return false;
```

**Code A Analysis:**
- Algorithm/Approach: Linear search without early exit
- Time Complexity: O(n) always
- Key Operations: Always traverses entire array

**Code B Analysis:**
- Algorithm/Approach: Linear search with early exit
- Time Complexity: O(n) worst case, O(1) best case
- Key Operations: Returns immediately when found

**Answer: B**
"""
        else:
            # æ— CoTç‰ˆæœ¬ - åªç»™ç­”æ¡ˆ
            return """
### Example 1:

Code A:
```cpp
int sum = 0;
for (int i = 0; i < n; i++) {
    sum += arr[i];
}
```

Code B:
```cpp
int sum = 0;
for (int i = 0; i < n; i++) {
    for (int j = 0; j <= i; j++) {
        if (j == i) sum += arr[i];
    }
}
```

**Answer: A**

### Example 2:

Code A:
```cpp
bool found = false;
for (int i = 0; i < n; i++) {
    if (arr[i] == target) found = true;
}
return found;
```

Code B:
```cpp
for (int i = 0; i < n; i++) {
    if (arr[i] == target) return true;
}
return false;
```

**Answer: B**
"""

    @staticmethod
    def _get_python_examples(with_cot: bool = True) -> str:
        """Python Few-Shot ç¤ºä¾‹"""
        if with_cot:
            return """
### Example 1:

Code A:
```python
total = 0
for i in range(len(arr)):
    total += arr[i]
```

Code B:
```python
total = 0
for i in range(len(arr)):
    for j in range(i + 1):
        if j == i:
            total += arr[i]
```

**Code A Analysis:**
- Algorithm/Approach: Direct summation with index iteration
- Time Complexity: O(n)
- Key Operations: n additions

**Code B Analysis:**
- Algorithm/Approach: Unnecessary nested loop
- Time Complexity: O(nÂ²)
- Key Operations: nÂ²/2 comparisons + n additions

**Answer: A**

### Example 2:

Code A:
```python
def find_target(arr, target):
    found = False
    for x in arr:
        if x == target:
            found = True
    return found
```

Code B:
```python
def find_target(arr, target):
    for x in arr:
        if x == target:
            return True
    return False
```

**Code A Analysis:**
- Algorithm/Approach: Linear search without early exit
- Time Complexity: O(n) always
- Key Operations: Always traverses entire array

**Code B Analysis:**
- Algorithm/Approach: Linear search with early exit
- Time Complexity: O(n) worst case, O(1) best case
- Key Operations: Returns immediately when found

**Answer: B**
"""
        else:
            return """
### Example 1:

Code A:
```python
total = 0
for i in range(len(arr)):
    total += arr[i]
```

Code B:
```python
total = 0
for i in range(len(arr)):
    for j in range(i + 1):
        if j == i:
            total += arr[i]
```

**Answer: A**

### Example 2:

Code A:
```python
def find_target(arr, target):
    found = False
    for x in arr:
        if x == target:
            found = True
    return found
```

Code B:
```python
def find_target(arr, target):
    for x in arr:
        if x == target:
            return True
    return False
```

**Answer: B**
"""

    @staticmethod
    def _get_java_examples(with_cot: bool = True) -> str:
        """Java Few-Shot ç¤ºä¾‹"""
        if with_cot:
            return """
### Example 1:

Code A:
```java
int sum = 0;
for (int i = 0; i < arr.length; i++) {
    sum += arr[i];
}
```

Code B:
```java
int sum = 0;
for (int i = 0; i < arr.length; i++) {
    for (int j = 0; j <= i; j++) {
        if (j == i) sum += arr[i];
    }
}
```

**Code A Analysis:**
- Algorithm/Approach: Direct summation
- Time Complexity: O(n)
- Key Operations: n additions

**Code B Analysis:**
- Algorithm/Approach: Unnecessary nested loop
- Time Complexity: O(nÂ²)
- Key Operations: nÂ²/2 comparisons + n additions

**Answer: A**

### Example 2:

Code A:
```java
public boolean findTarget(int[] arr, int target) {
    boolean found = false;
    for (int x : arr) {
        if (x == target) found = true;
    }
    return found;
}
```

Code B:
```java
public boolean findTarget(int[] arr, int target) {
    for (int x : arr) {
        if (x == target) return true;
    }
    return false;
}
```

**Code A Analysis:**
- Algorithm/Approach: Linear search without early exit
- Time Complexity: O(n) always
- Key Operations: Always traverses entire array

**Code B Analysis:**
- Algorithm/Approach: Linear search with early exit
- Time Complexity: O(n) worst case, O(1) best case
- Key Operations: Returns immediately when found

**Answer: B**
"""
        else:
            return """
### Example 1:

Code A:
```java
int sum = 0;
for (int i = 0; i < arr.length; i++) {
    sum += arr[i];
}
```

Code B:
```java
int sum = 0;
for (int i = 0; i < arr.length; i++) {
    for (int j = 0; j <= i; j++) {
        if (j == i) sum += arr[i];
    }
}
```

**Answer: A**

### Example 2:

Code A:
```java
public boolean findTarget(int[] arr, int target) {
    boolean found = false;
    for (int x : arr) {
        if (x == target) found = true;
    }
    return found;
}
```

Code B:
```java
public boolean findTarget(int[] arr, int target) {
    for (int x : arr) {
        if (x == target) return true;
    }
    return false;
}
```

**Answer: B**
"""
    
    # ==================== åŒä»£ç æ¨¡æ¿ (A/B/Similar) ====================
    
    @staticmethod
    def zero_shot_pair(code_a: str, code_b: str, lang: str) -> dict:
        """
        Zero-Shot (ZS): Direct comparison without examples or reasoning
        The model is asked to directly compare two functionally equivalent implementations
        and determine which one is more efficient. Outputs only the identifier (A or B).
        """
        lang_display = PromptTemplates.get_lang_display(lang)
        
        system_prompt = f"""You are a senior Performance Engineer with 10+ years of experience in {lang_display} optimization.

Your task is to determine which code runs faster based on algorithmic complexity, data structure efficiency, and implementation details."""
        
        user_prompt = f"""Compare the following two functionally equivalent {lang_display} implementations and determine which one is more efficient.

## Code A:
```{lang}
{code_a}
```

## Code B:
```{lang}
{code_b}
```

Based on algorithm complexity, data structures, loop efficiency, and other performance metrics, determine which implementation runs faster.

You must output ONLY the identifier of the faster implementation:
- Output "A" if Code A is faster
- Output "B" if Code B is faster

Your response must be exactly one character: A or B

No explanation or additional text."""
        
        return {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "expect_cot": False
        }
    
    @staticmethod
    def zero_shot_cot_pair(code_a: str, code_b: str, lang: str) -> dict:
        """
        Zero-Shot CoT: æ— ç¤ºä¾‹ï¼Œä½†è¦æ±‚æ¨ç†è¿‡ç¨‹
        """
        lang_display = PromptTemplates.get_lang_display(lang)
        
        system_prompt = f"""You are a senior Performance Engineer with 10+ years of experience in {lang_display} optimization.

Your task is to determine which code runs faster based on algorithmic complexity, data structure efficiency, and implementation details.

CRITICAL OUTPUT FORMAT REQUIREMENT:
You MUST end your response with EXACTLY this format:
**Answer: X**

Where X is either 'A' or 'B' (single letter, no other text).
This line must be the LAST line of your response.
Do NOT add any text after the answer line."""
        
        user_prompt = f"""Determine which of the following two {lang_display} code snippets runs faster.

## Code A:
```{lang}
{code_a}
```

## Code B:
```{lang}
{code_b}
```

These two code snippets have different performance characteristics. Analyze using the following format:

**Code A Analysis:**
- Algorithm/Approach:
- Time Complexity:
- Key Operations and their costs:

**Code B Analysis:**
- Algorithm/Approach:
- Time Complexity:
- Key Operations and their costs:

**Comparison Conclusion:**
Based on the analysis above, determine which code is faster. You must choose one:
- A: Code A is faster
- B: Code B is faster

You MUST end your response with EXACTLY this format on the last line:
**Answer: A or B**"""
        
        return {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "expect_cot": True
        }
    
    @staticmethod
    def few_shot_pair(code_a: str, code_b: str, lang: str) -> dict:
        """
        Few-Shot (FS): With 2 in-context examples showing only input and output
        The prompt includes two in-context examples that demonstrate correct efficiency
        comparisons between code pairs. Each example presents the input code snippets
        and the final decision, but does not expose intermediate reasoning steps.
        Always uses language-specific examples matching the target language.
        """
        lang_display = PromptTemplates.get_lang_display(lang)
        
        # å§‹ç»ˆä½¿ç”¨è¯­è¨€ç‰¹å®šçš„ç¤ºä¾‹
        examples = PromptTemplates.get_examples_for_lang(lang, with_cot=False)
        
        system_prompt = f"""You are a senior Performance Engineer with 10+ years of experience in {lang_display} optimization.

Your task is to determine which code runs faster based on algorithmic complexity, data structure efficiency, and implementation details."""
        
        user_prompt = f"""Compare the following two functionally equivalent {lang_display} implementations and determine which one is more efficient.

Here are two examples of correct efficiency comparisons:
{examples}

### Now analyze the following {lang_display} code:

Code A:
```{lang}
{code_a}
```

Code B:
```{lang}
{code_b}
```

Based on algorithm complexity, data structures, and implementation efficiency, determine which implementation runs faster.

You must output ONLY the identifier:
- Output "A" if Code A is faster
- Output "B" if Code B is faster

Your response must be exactly one character: A or B"""
        
        return {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "expect_cot": False
        }
    
    @staticmethod
    def few_shot_cot_pair(code_a: str, code_b: str, lang: str) -> dict:
        """
        Few-Shot Chain-of-Thought (FS-CoT): With explicit analytical reasoning
        The prompt includes two in-context examples with explicit analytical reasoning.
        Each example illustrates a step-by-step efficiency analysis, including algorithm
        identification, complexity analysis, and key operation counting, followed by the
        final comparison outcome.
        Always uses language-specific examples matching the target language.
        """
        lang_display = PromptTemplates.get_lang_display(lang)
        
        # å§‹ç»ˆä½¿ç”¨è¯­è¨€ç‰¹å®šçš„ç¤ºä¾‹
        examples = PromptTemplates.get_examples_for_lang(lang, with_cot=True)
        
        system_prompt = f"""You are a senior Performance Engineer with 10+ years of experience in {lang_display} optimization.

Your task is to determine which code runs faster based on algorithmic complexity, data structure efficiency, and implementation details.

CRITICAL OUTPUT FORMAT REQUIREMENT:
You MUST end your response with EXACTLY this format:
**Answer: X**

Where X is either 'A' or 'B' (single letter, no other text).
This line must be the LAST line of your response.
Do NOT add any text after the answer line."""
        
        user_prompt = f"""Compare the following two functionally equivalent {lang_display} implementations and determine which one is more efficient.

Here are two examples with step-by-step reasoning:
{examples}

### Now analyze the following {lang_display} code:

Code A:
```{lang}
{code_a}
```

Code B:
```{lang}
{code_b}
```

Provide a step-by-step efficiency analysis using the following format:

**Code A Analysis:**
- Algorithm/Approach: [describe the algorithmic approach]
- Time Complexity: [Big-O notation]
- Key Operations: [count or describe critical operations]

**Code B Analysis:**
- Algorithm/Approach: [describe the algorithmic approach]
- Time Complexity: [Big-O notation]
- Key Operations: [count or describe critical operations]

**Answer: [A or B]**

Remember: Your LAST line must be exactly "**Answer: X**" where X is A or B."""

        return {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "expect_cot": True
        }
    
    # ==================== ä¸‰å…ƒç»„æ¨¡æ¿ (A/B/C) ====================
    
    @staticmethod
    def zero_shot_triple(code_a: str, code_b: str, code_c: str, lang: str) -> dict:
        """
        Zero-Shot v8: Triple comparison
        Determine which code is fastest
        """
        lang_display = PromptTemplates.get_lang_display(lang)
        
        system_prompt = f"""You are a senior Performance Engineer with 10+ years of experience in {lang_display} optimization.

Your task is to determine which code runs fastest based on algorithmic complexity, data structure efficiency, and implementation details."""
        
        user_prompt = f"""Determine which of the following three {lang_display} code snippets runs fastest.

## Code A:
```{lang}
{code_a}
```

## Code B:
```{lang}
{code_b}
```

## Code C:
```{lang}
{code_c}
```

These three code snippets have different performance characteristics. Based on algorithm complexity, data structures, and implementation efficiency, determine which one is fastest.

You must choose one:
- A: Code A is fastest
- B: Code B is fastest
- C: Code C is fastest

Respond with only "A", "B", or "C", no explanation."""
        
        return {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "expect_cot": False
        }
    
    @staticmethod
    def zero_shot_cot_triple(code_a: str, code_b: str, code_c: str, lang: str) -> dict:
        """
        Zero-Shot CoT Triple: æ— ç¤ºä¾‹ï¼Œä½†è¦æ±‚æ¨ç†è¿‡ç¨‹
        """
        lang_display = PromptTemplates.get_lang_display(lang)
        
        system_prompt = f"""You are a senior Performance Engineer with 10+ years of experience in {lang_display} optimization.

Your task is to determine which code runs fastest based on algorithmic complexity, data structure efficiency, and implementation details.

CRITICAL OUTPUT FORMAT REQUIREMENT:
You MUST end your response with EXACTLY this format:
**Answer: X**

Where X is either 'A', 'B', or 'C' (single letter, no other text).
This line must be the LAST line of your response.
Do NOT add any text after the answer line."""
        
        user_prompt = f"""Determine which of the following three {lang_display} code snippets runs fastest.

## Code A:
```{lang}
{code_a}
```

## Code B:
```{lang}
{code_b}
```

## Code C:
```{lang}
{code_c}
```

These three code snippets have different performance characteristics. Analyze using the following format:

**Code A Analysis:**
- Algorithm/Approach:
- Time Complexity:
- Key Operations and their costs:

**Code B Analysis:**
- Algorithm/Approach:
- Time Complexity:
- Key Operations and their costs:

**Code C Analysis:**
- Algorithm/Approach:
- Time Complexity:
- Key Operations and their costs:

**Comparison Conclusion:**
Based on the analysis above, determine which code is fastest.

You MUST end your response with EXACTLY this format on the last line:
**Answer: A, B, or C**"""
        
        return {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "expect_cot": True
        }
    
    @staticmethod
    def few_shot_triple(code_a: str, code_b: str, code_c: str, lang: str) -> dict:
        """
        Few-Shot w/o CoT Triple: æœ‰ç¤ºä¾‹ï¼Œä½†ä¸è¦æ±‚æ¨ç†è¿‡ç¨‹
        """
        lang_display = PromptTemplates.get_lang_display(lang)
        
        # Tripleç¤ºä¾‹ï¼ˆæ— CoTï¼‰
        examples = """
### Example:

Code A:
```cpp
int sum = 0;
for (int i = 0; i < n; i++) sum += arr[i];
```

Code B:
```cpp
int sum = 0;
for (int i = 0; i < n; i++) {
    for (int j = 0; j <= i; j++) {
        if (j == i) sum += arr[i];
    }
}
```

Code C:
```cpp
int sum = 0;
for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
        if (i == j) sum += arr[i];
    }
}
```

**Answer: A**
"""
        
        system_prompt = f"""You are a senior Performance Engineer with 10+ years of experience in {lang_display} optimization.

Your task is to determine which code runs fastest based on algorithmic complexity, data structure efficiency, and implementation details."""
        
        user_prompt = f"""Determine which of the following three {lang_display} code snippets runs fastest.

Here is an example:
{examples}

### Now analyze the following {lang_display} code:

Code A:
```{lang}
{code_a}
```

Code B:
```{lang}
{code_b}
```

Code C:
```{lang}
{code_c}
```

Based on algorithm complexity, data structures, and implementation efficiency, determine which one is fastest.

You must choose one:
- A: Code A is fastest
- B: Code B is fastest
- C: Code C is fastest

Respond with only "A", "B", or "C", no explanation."""
        
        return {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "expect_cot": False
        }
    
    @staticmethod
    def few_shot_cot_triple(code_a: str, code_b: str, code_c: str, lang: str) -> dict:
        """
        Few-Shot CoT v8: Triple comparison with reasoning
        Determine which code is fastest through step-by-step analysis
        """
        lang_display = PromptTemplates.get_lang_display(lang)
        
        system_prompt = f"""You are a senior Performance Engineer with 10+ years of experience in {lang_display} optimization.

Your task is to determine which code runs fastest based on algorithmic complexity, data structure efficiency, and implementation details.

CRITICAL OUTPUT FORMAT REQUIREMENT:
You MUST end your response with EXACTLY this format:
**Answer: X**

Where X is either 'A', 'B', or 'C' (single letter, no other text).
This line must be the LAST line of your response.
Do NOT add any text after the answer line."""
        
        user_prompt = f"""You are a code performance analysis expert. Analyze the following three code snippets and determine which one runs fastest.

### Analyze the following {lang_display} code:

Code A:
```{lang}
{code_a}
```

Code B:
```{lang}
{code_b}
```

Code C:
```{lang}
{code_c}
```

These three code snippets solve the same problem but have different performance characteristics. Analyze using the following format:

**Code A Analysis:**
- Algorithm/Approach:
- Time Complexity:
- Key Operations and their costs:

**Code B Analysis:**
- Algorithm/Approach:
- Time Complexity:
- Key Operations and their costs:

**Code C Analysis:**
- Algorithm/Approach:
- Time Complexity:
- Key Operations and their costs:

**Comparison Conclusion:**
Based on the analysis above, determine which code is fastest.

CRITICAL: Your response MUST end with EXACTLY this format on the last line:
**Answer: A**
or
**Answer: B**
or
**Answer: C**

REQUIREMENTS:
1. The answer line must be the LAST line of your response
2. Use exactly "**Answer: A**" or "**Answer: B**" or "**Answer: C**" (replace with your choice)
3. Do NOT write anything after this line
4. Do NOT add explanations, periods, or any other characters after the letter
5. The letter must be A, B, or C only (uppercase, single character)

EXAMPLE OF CORRECT FORMAT:
**Code A Analysis:**
...
**Code B Analysis:**
...
**Code C Analysis:**
...
**Comparison Conclusion:**
...
**Answer: C**

[END OF RESPONSE - Nothing should appear after the Answer line]"""

        return {
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "expect_cot": True
        }
    

# ==================== ç­”æ¡ˆæå– ====================
def extract_answer(response: str, is_triple: bool = False) -> Tuple[str, str]:
    """
    ä»å“åº”çš„æœ€åä¸€è¡Œæå–ç­”æ¡ˆ
    
    åªæ¥å—æ ¼å¼: **Answer: X** æˆ– Answer: X
    å…¶ä¸­ X ä¸º A, B, C (æˆ– Similar)
    """
    response_clean = response.strip()
    
    if not response_clean:
        return "UNKNOWN", response_clean
    
    # åªæ£€æŸ¥æœ€åä¸€è¡Œ
    last_line = response_clean.split('\n')[-1].strip()
    
    # åŒ¹é… **Answer: X** æˆ– Answer: X æ ¼å¼
    # æ”¯æŒä¸­è‹±æ–‡ï¼Œæ”¯æŒå¸¦/ä¸å¸¦æ˜Ÿå·
    pattern = r'\*?\*?(?:Answer|ç­”æ¡ˆ)[ï¼š:\s]*([ABC]|Similar|å·®ä¸å¤š)\*?\*?'
    match = re.search(pattern, last_line, re.IGNORECASE)
    
    if match:
        answer = match.group(1).upper()
        
        # ä¸­æ–‡æ˜ å°„
        if answer == "å·®ä¸å¤š":
            answer = "SIMILAR"
        
        # æ ‡å‡†åŒ–
        valid_answers = ["A", "B", "C", "SIMILAR"]
        if answer in valid_answers:
            # Pair æ¨¡å¼ä¸åº”è¯¥æœ‰ C
            if not is_triple and answer == "C":
                return "PARSE_FAILED", response_clean
            
            return answer, response_clean
    
    # å¦‚æœæœ€åä¸€è¡ŒåªåŒ…å«å•ä¸ªå­—æ¯ A/B/C (å»é™¤æ‰€æœ‰ç¬¦å·å)
    cleaned_last = last_line.upper().replace('*', '').replace('.', '').replace(':', '').replace(' ', '')
    if cleaned_last in ["A", "B", "C"]:
        answer = cleaned_last
        # Pair æ¨¡å¼ä¸åº”è¯¥æœ‰ C
        if not is_triple and answer == "C":
            return "PARSE_FAILED", response_clean
        return answer, response_clean
    
    # æœªèƒ½ä»æœ€åä¸€è¡Œæå–ç­”æ¡ˆ - æ ‡è®°ä¸ºè§£æå¤±è´¥
    return "PARSE_FAILED", response_clean


# ==================== æ•°æ®ç±» ====================
@dataclass
class PairEvalResult:
    """åŒä»£ç è¯„ä¼°ç»“æœ"""
    pair_id: str
    problem_id: str = ""
    language: str = "cpp"
    dataset_type: str = ""
    prompt_type: str = "zero-shot"
    expected_answer_type: str = "fast"
    
    # Speedup bin ä¿¡æ¯ (æ¥è‡ªæ•°æ®é›†æ„å»º)
    speedup_bin: str = ""
    speedup_bin_idx: int = -1
    
    original_slow_time: float = 0
    original_fast_time: float = 0
    original_speedup: Optional[float] = None
    
    test1_order: str = "slow_A_fast_B"
    test1_correct_answer: str = "B"
    test1_llm_prediction: str = ""
    test1_llm_raw_response: str = ""
    test1_reasoning_trace: str = ""
    test1_correct: Optional[bool] = None
    test1_prompt_tokens: int = 0
    test1_completion_tokens: int = 0
    test1_total_tokens: int = 0
    
    test2_order: str = "fast_A_slow_B"
    test2_correct_answer: str = "A"
    test2_llm_prediction: str = ""
    test2_llm_raw_response: str = ""
    test2_reasoning_trace: str = ""
    test2_correct: Optional[bool] = None
    test2_prompt_tokens: int = 0
    test2_completion_tokens: int = 0
    test2_total_tokens: int = 0
    
    is_consistent: Optional[bool] = None
    category: str = ""
    
    error: Optional[str] = None


@dataclass
class TripleEvalResult:
    """ä¸‰å…ƒç»„è¯„ä¼°ç»“æœ"""
    pair_id: str
    problem_id: str = ""
    language: str = "cpp"
    dataset_type: str = "triple"
    prompt_type: str = "zero-shot"
    expected_answer_type: str = "fast"
    
    fast_time: float = 0
    medium_time: float = 0
    slow_time: float = 0
    speedup_fast_vs_slow: Optional[float] = None
    
    test1_order: str = "fast_A_medium_B_slow_C"
    test1_correct_answer: str = "A"
    test1_llm_prediction: str = ""
    test1_llm_raw_response: str = ""
    test1_reasoning_trace: str = ""
    test1_correct: Optional[bool] = None
    test1_prompt_tokens: int = 0
    test1_completion_tokens: int = 0
    test1_total_tokens: int = 0
    
    test2_order: str = "slow_A_fast_B_medium_C"
    test2_correct_answer: str = "B"
    test2_llm_prediction: str = ""
    test2_llm_raw_response: str = ""
    test2_reasoning_trace: str = ""
    test2_correct: Optional[bool] = None
    test2_prompt_tokens: int = 0
    test2_completion_tokens: int = 0
    test2_total_tokens: int = 0
    
    test3_order: str = "medium_A_slow_B_fast_C"
    test3_correct_answer: str = "C"
    test3_llm_prediction: str = ""
    test3_llm_raw_response: str = ""
    test3_reasoning_trace: str = ""
    test3_correct: Optional[bool] = None
    test3_prompt_tokens: int = 0
    test3_completion_tokens: int = 0
    test3_total_tokens: int = 0
    
    num_correct: int = 0
    is_all_correct: bool = False
    is_consistent: Optional[bool] = None
    category: str = ""
    
    error: Optional[str] = None


# ==================== LLM å®¢æˆ·ç«¯ ====================
client = OpenAI(api_key=API_KEY, base_url=BASE_URL)


def ask_llm(codes: List[str], lang: str, prompt_type: str, 
            is_triple: bool = False, retry: int = 3) -> dict:
    """
    ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£ - v9 ç®€åŒ–ç‰ˆ
    
    æ”¯æŒçš„ prompt_type:
    - zero-shot: æ— ç¤ºä¾‹ï¼Œæ— CoT
    - few-shot: æœ‰ç¤ºä¾‹ï¼ˆè¯­è¨€ç‰¹å®šï¼‰ï¼Œæ— CoT
    - few-shot-cot: æœ‰ç¤ºä¾‹ï¼ˆè¯­è¨€ç‰¹å®šï¼‰ï¼Œæœ‰CoT
    
    åªæ”¯æŒ pair-wise æ¯”è¾ƒï¼ˆis_triple å¿…é¡»ä¸º Falseï¼‰
    
    è¿”å›å€¼åŒ…å«:
    - prediction: é¢„æµ‹ç»“æœ (A/B/PARSE_FAILED/ERROR)
    - raw_response: åŸå§‹å“åº”
    - reasoning_trace: æ¨ç†è¿‡ç¨‹
    - prompt_tokens: è¾“å…¥tokenæ•°
    - completion_tokens: è¾“å‡ºtokenæ•°
    - total_tokens: æ€»tokenæ•°
    """
    
    if is_triple:
        raise ValueError("Triple comparison is no longer supported in v9")
    
    # ==================== ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡Prompt ====================
    code_a, code_b = codes[0], codes[1]
    
    # è®°å½•ä»£ç é•¿åº¦
    total_len = len(code_a) + len(code_b)
    if total_len > 10000:
        print(f"[INFO] Large code pair ({total_len} chars)")
    
    # æ ¹æ®promptç±»å‹é€‰æ‹©æ¨¡æ¿
    if prompt_type == "zero-shot":
        prompt_data = PromptTemplates.zero_shot_pair(code_a, code_b, lang)
    elif prompt_type == "few-shot":
        prompt_data = PromptTemplates.few_shot_pair(code_a, code_b, lang)
    elif prompt_type == "few-shot-cot":
        prompt_data = PromptTemplates.few_shot_cot_pair(code_a, code_b, lang)
    else:
        raise ValueError(f"Unknown prompt type: {prompt_type}. Supported types: zero-shot, few-shot, few-shot-cot")
    
    # ==================== ç¬¬äºŒæ­¥ï¼šè®¾ç½®å‚æ•° ====================
    expect_cot = prompt_data.get("expect_cot", False)
    
    # æ ¹æ®promptç±»å‹è®¾ç½®max_tokens
    if expect_cot:
        max_tokens = 8000  # Few-shotéœ€è¦è¯¦ç»†åˆ†æ
    else:
        max_tokens = 8000  # Zero-shotåªéœ€è¦ç®€çŸ­å›ç­”
    
    # ==================== ç¬¬ä¸‰æ­¥ï¼šé‡è¯•å¾ªç¯ ====================
    for attempt in range(retry):
        try:
            # æ„å»ºAPIå‚æ•°
            api_params = {
                "model": MODEL,
                "messages": prompt_data["messages"],
                "temperature": 0.1,
                "timeout": 180
            }
            
            # ğŸ”¥ å…³é”®ä¿®å¤1: æ ¹æ®æ¨¡å‹é€‰æ‹©æ­£ç¡®çš„å‚æ•°å
            # GPT-5 ç³»åˆ—ä½¿ç”¨ max_completion_tokensï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨ max_tokens
            if "gpt-5" in MODEL.lower() or "gpt5" in MODEL.lower():
                api_params["max_completion_tokens"] = max_tokens
            else:
                api_params["max_tokens"] = max_tokens
            
            # è°ƒç”¨API
            resp = client.chat.completions.create(**api_params)
            
            # ğŸ”¥ å®¹é”™ä¿æŠ¤ï¼šæ£€æŸ¥å“åº”å¯¹è±¡ç±»å‹ï¼Œé˜²æ­¢æŸäº›ä¸­è½¬ç«™è¿”å›å­—ç¬¦ä¸²å¯¼è‡´å´©æºƒ
            if isinstance(resp, str):
                raise ValueError(f"API returned a string instead of an object: {resp[:100]}")
            
            if not hasattr(resp, 'choices') or not resp.choices:
                raise ValueError(f"API returned an invalid response object: {type(resp)}")
            
            # ğŸ”¥ å…³é”®ä¿®å¤3: æ£€æŸ¥contentæ˜¯å¦ä¸ºNone
            message_content = resp.choices[0].message.content
            finish_reason = resp.choices[0].finish_reason
            
            # å¤„ç†ç©ºå“åº”
            if message_content is None or message_content.strip() == "":
                error_msg = f"Empty content (finish_reason: {finish_reason})"
                print(f"[WARN] Attempt {attempt+1}/{retry}: {error_msg}")
                
                # å¦‚æœæ˜¯å†…å®¹è¿‡æ»¤ï¼Œç›´æ¥è¿”å›é”™è¯¯ï¼Œä¸é‡è¯•
                if finish_reason == "content_filter":
                    return {
                        "prediction": "ERROR",
                        "raw_response": "Content filtered by API",
                        "reasoning_trace": "",
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0
                    }
                
                # å¦‚æœæ˜¯å…¶ä»–åŸå› çš„ç©ºå“åº”ï¼Œé‡è¯•
                if attempt < retry - 1:
                    print(f"[RETRY] Retrying after empty response...")
                    time.sleep(3 * (attempt + 1))  # é€’å¢ç­‰å¾…æ—¶é—´
                    continue
                
                # æœ€åä¸€æ¬¡é‡è¯•ä¹Ÿå¤±è´¥äº†
                return {
                    "prediction": "ERROR",
                    "raw_response": error_msg,
                    "reasoning_trace": "",
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0
                }
            
            # è·å–å“åº”å†…å®¹
            raw_response = message_content.strip()
            
            # ğŸ”¥ å…³é”®ä¿®å¤4: å¤„ç†å“åº”è¢«æˆªæ–­çš„æƒ…å†µ
            if finish_reason == "length":
                print(f"[WARN] Response truncated at {max_tokens} tokens")
                prediction, reasoning_trace = extract_answer(raw_response, is_triple)
                
                # å¦‚æœæå–å¤±è´¥ä¸”è¿˜æœ‰é‡è¯•æœºä¼šï¼Œå¢åŠ max_tokensé‡è¯•
                if prediction == "UNKNOWN" and attempt < retry - 1:
                    new_max_tokens = int(max_tokens * 1.5)
                    print(f"[RETRY] Increasing max_tokens from {max_tokens} to {new_max_tokens}")
                    max_tokens = new_max_tokens
                    time.sleep(2)
                    continue
                
                # å¦‚æœæˆåŠŸæå–æˆ–å·²æ˜¯æœ€åä¸€æ¬¡é‡è¯•ï¼Œè¿”å›ç»“æœ
                usage = resp.usage if hasattr(resp, 'usage') else None
                return {
                    "prediction": prediction,
                    "raw_response": raw_response + "\n\n[RESPONSE TRUNCATED]",
                    "reasoning_trace": reasoning_trace if expect_cot else "",
                    "prompt_tokens": usage.prompt_tokens if usage else 0,
                    "completion_tokens": usage.completion_tokens if usage else 0,
                    "total_tokens": usage.total_tokens if usage else 0
                }
            
            # ==================== æ­£å¸¸æƒ…å†µï¼šæå–ç­”æ¡ˆ ====================
            prediction, reasoning_trace = extract_answer(raw_response, is_triple)
            
            # å¦‚æœæå–å¤±è´¥ä½†å“åº”çœ‹èµ·æ¥æ­£å¸¸ï¼Œè®°å½•è­¦å‘Š
            if prediction == "PARSE_FAILED":
                print(f"[WARN] Answer parse failed - last line: {raw_response.split(chr(10))[-1][:100]}")
            
            # è·å–tokenä½¿ç”¨ä¿¡æ¯
            usage = resp.usage if hasattr(resp, 'usage') else None
            
            return {
                "prediction": prediction,
                "raw_response": raw_response,
                "reasoning_trace": reasoning_trace if expect_cot else "",
                "prompt_tokens": usage.prompt_tokens if usage else 0,
                "completion_tokens": usage.completion_tokens if usage else 0,
                "total_tokens": usage.total_tokens if usage else 0
            }
            
        except Exception as e:
            error_msg = f"Attempt {attempt + 1}/{retry} failed: {str(e)}"
            print(f"[ERROR] {error_msg}")
            
            # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œç»§ç»­
            if attempt < retry - 1:
                wait_time = 3 * (attempt + 1)
                print(f"[RETRY] Waiting {wait_time} seconds before retry...")
                time.sleep(wait_time)
                continue
            
            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
            return {
                "prediction": "ERROR",
                "raw_response": str(e)[:500],  # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦
                "reasoning_trace": ""
            }
    
    # ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¿™é‡Œï¼Œä½†ä»¥é˜²ä¸‡ä¸€
    return {
        "prediction": "ERROR",
        "raw_response": "Max retries exceeded",
        "reasoning_trace": "",
        "prompt_tokens": 0,
        "completion_tokens": 0,
        "total_tokens": 0
    }

# ==================== å¤„ç†å‡½æ•° ====================
def process_pair(data: dict, prompt_type: str = "zero-shot") -> PairEvalResult:
    """å¤„ç†åŒä»£ç é…å¯¹"""
    
    # ä½¿ç”¨é¢„ç”Ÿæˆçš„ pair_idï¼Œæˆ–è‡ªè¡Œç”Ÿæˆ
    pair_id = data.get("_generated_pair_id") or data.get("pair_id")
    if not pair_id:
        problem_id = data.get("problem_id", "")
        speedup = data.get("speedup", "")
        pair_id = f"{problem_id}_{speedup}" if problem_id else "unknown"
    problem_id = data.get("problem_id", "")
    dataset_type = detect_dataset_type(data)
    expected_answer_type = get_expected_answer_type(data)
    lang = data.get("language", "cpp").lower()
    
    if dataset_type == "clean_vs_raw":
        code_slow = data.get("raw_code", "")
        code_fast = data.get("clean_code", "")
        slow_time = data.get("cpu_time", 0)
        fast_time = data.get("cpu_time", 0)
    elif dataset_type == "same_tier":
        code_slow = data.get("code_b", "")
        code_fast = data.get("code_a", "")
        slow_time = data.get("time_b", 0)
        fast_time = data.get("time_a", 0)
    elif dataset_type == "effibench_pair":
        code_slow = data.get("src_code", "")
        code_fast = data.get("tgt_code", "")
        try:
            slow_time = float(data.get("src_agg_runtime", 0) or 0)
            fast_time = float(data.get("tgt_agg_runtime", 0) or 0)
        except ValueError:
            slow_time, fast_time = 0, 0
    else:
        code_slow = data.get("slow_code", data.get("code_b", ""))
        code_fast = data.get("fast_code", data.get("code_a", ""))
        slow_time = data.get("slow_time", data.get("time_b", 0))
        fast_time = data.get("fast_time", data.get("time_a", 0))
    
    if expected_answer_type == "similar":
        test1_correct_answer = "SIMILAR"
        test2_correct_answer = "SIMILAR"
    else:
        test1_correct_answer = "B"
        test2_correct_answer = "A"
    
    # è‡ªåŠ¨è®¡ç®— speedup_binï¼ˆå¦‚æœæ•°æ®ä¸­æ²¡æœ‰ï¼‰
    # ä½¿ç”¨ç»Ÿä¸€çš„éš¾åº¦ç­‰çº§åˆ†æ¡£: hard=[2,4), medium=[4,8), easy=[8,inf)
    speedup_bin = data.get("speedup_bin", "")
    speedup_bin_idx = data.get("speedup_bin_idx", -1)
    
    if not speedup_bin and fast_time > 0:
        speedup = data.get("speedup") or (slow_time / fast_time)
        bins = [2.0, 4.0, 8.0, float('inf')]
        difficulty_map = {0: 'hard', 1: 'medium', 2: 'easy'}
        
        for i in range(len(bins) - 1):
            if bins[i] <= speedup < bins[i + 1]:
                difficulty = difficulty_map.get(i, f'level_{i}')
                if bins[i + 1] == float('inf'):
                    speedup_bin = f"{difficulty} [â‰¥{bins[i]}x]"
                else:
                    speedup_bin = f"{difficulty} [{bins[i]}x-{bins[i+1]}x)"
                speedup_bin_idx = i
                break
        
        # å¦‚æœ speedup < 2ï¼Œå½’å…¥ hard
        if not speedup_bin and speedup >= 1:
            speedup_bin = "below_threshold"
            speedup_bin_idx = -1
    
    result = PairEvalResult(
        pair_id=str(pair_id),
        problem_id=str(problem_id),
        language=lang,
        dataset_type=dataset_type,
        prompt_type=prompt_type,
        expected_answer_type=expected_answer_type,
        speedup_bin=speedup_bin,
        speedup_bin_idx=speedup_bin_idx,
        original_slow_time=slow_time,
        original_fast_time=fast_time,
        original_speedup=round(slow_time / fast_time, 2) if fast_time > 0 else None,
        test1_correct_answer=test1_correct_answer,
        test2_correct_answer=test2_correct_answer,
    )
    
    if not code_slow or not code_fast:
        result.error = "missing_code"
        return result
    
    # Test 1: slow=A, fast=B
    llm_result1 = ask_llm([code_slow, code_fast], lang, prompt_type, is_triple=False)
    result.test1_llm_prediction = llm_result1["prediction"]
    result.test1_llm_raw_response = llm_result1["raw_response"]
    result.test1_reasoning_trace = llm_result1["reasoning_trace"]
    result.test1_prompt_tokens = llm_result1.get("prompt_tokens", 0)
    result.test1_completion_tokens = llm_result1.get("completion_tokens", 0)
    result.test1_total_tokens = llm_result1.get("total_tokens", 0)
    
    if llm_result1["prediction"] in ["A", "B"]:
        result.test1_correct = (llm_result1["prediction"] == test1_correct_answer)
    elif llm_result1["prediction"] == "PARSE_FAILED":
        result.error = "test1_answer_parse_failed"
        return result
    else:
        result.error = f"test1_error: {llm_result1['raw_response'][:100]}"
        return result
    
    time.sleep(0.3)
    
    # Test 2: fast=A, slow=B
    llm_result2 = ask_llm([code_fast, code_slow], lang, prompt_type, is_triple=False)
    result.test2_llm_prediction = llm_result2["prediction"]
    result.test2_llm_raw_response = llm_result2["raw_response"]
    result.test2_reasoning_trace = llm_result2["reasoning_trace"]
    result.test2_prompt_tokens = llm_result2.get("prompt_tokens", 0)
    result.test2_completion_tokens = llm_result2.get("completion_tokens", 0)
    result.test2_total_tokens = llm_result2.get("total_tokens", 0)
    
    if llm_result2["prediction"] in ["A", "B"]:
        result.test2_correct = (llm_result2["prediction"] == test2_correct_answer)
    elif llm_result2["prediction"] == "PARSE_FAILED":
        result.error = "test2_answer_parse_failed"
        return result
    else:
        result.error = f"test2_error: {llm_result2['raw_response'][:100]}"
        return result
    
    # ç»¼åˆåˆ†æ
    test1_chose_fast = (result.test1_llm_prediction == "B")
    test2_chose_fast = (result.test2_llm_prediction == "A")
    result.is_consistent = (test1_chose_fast == test2_chose_fast)
    
    if result.test1_correct and result.test2_correct:
        result.category = "both_correct"
    elif not result.test1_correct and not result.test2_correct:
        result.category = "both_wrong"
    else:
        result.category = "position_bias"
    
    return result


def process_triple(data: dict, prompt_type: str = "zero-shot") -> TripleEvalResult:
    """å¤„ç†ä¸‰å…ƒç»„"""
    
    # ä½¿ç”¨é¢„ç”Ÿæˆçš„ pair_idï¼Œæˆ–è‡ªè¡Œç”Ÿæˆ
    pair_id = data.get("_generated_pair_id") or data.get("pair_id")
    if not pair_id:
        problem_id = data.get("problem_id", "")
        speedup = data.get("speedup", "")
        pair_id = f"{problem_id}_{speedup}" if problem_id else "unknown"
    problem_id = data.get("problem_id", "")
    lang = data.get("language", "cpp").lower()
    
    fast_code = data.get("fast_code", "")
    medium_code = data.get("medium_code", "")
    slow_code = data.get("slow_code", "")
    
    fast_time = data.get("fast_time", 0)
    medium_time = data.get("medium_time", 0)
    slow_time = data.get("slow_time", 0)
    
    result = TripleEvalResult(
        pair_id=str(pair_id),
        problem_id=str(problem_id),
        language=lang,
        prompt_type=prompt_type,
        expected_answer_type="fast",
        fast_time=fast_time,
        medium_time=medium_time,
        slow_time=slow_time,
        speedup_fast_vs_slow=round(slow_time / fast_time, 2) if fast_time > 0 else None
    )
    
    if not fast_code or not medium_code or not slow_code:
        result.error = "missing_code"
        return result
    
    # Test 1: fast=A, medium=B, slow=Cï¼Œæ­£ç¡®=A
    llm1 = ask_llm([fast_code, medium_code, slow_code], lang, prompt_type, is_triple=True)
    result.test1_llm_prediction = llm1["prediction"]
    result.test1_llm_raw_response = llm1["raw_response"]
    result.test1_reasoning_trace = llm1["reasoning_trace"]
    
    if llm1["prediction"] in ["A", "B", "C"]:
        result.test1_correct = (llm1["prediction"] == "A")
    elif llm1["prediction"] == "PARSE_FAILED":
        result.error = "test1_answer_parse_failed"
        return result
    else:
        result.error = f"test1_error"
        return result
    
    time.sleep(0.3)
    
    # Test 2: slow=A, fast=B, medium=Cï¼Œæ­£ç¡®=B
    llm2 = ask_llm([slow_code, fast_code, medium_code], lang, prompt_type, is_triple=True)
    result.test2_llm_prediction = llm2["prediction"]
    result.test2_llm_raw_response = llm2["raw_response"]
    result.test2_reasoning_trace = llm2["reasoning_trace"]
    
    if llm2["prediction"] in ["A", "B", "C"]:
        result.test2_correct = (llm2["prediction"] == "B")
    elif llm2["prediction"] == "PARSE_FAILED":
        result.error = "test2_answer_parse_failed"
        return result
    else:
        result.error = f"test2_error"
        return result
    
    time.sleep(0.3)
    
    # Test 3: medium=A, slow=B, fast=Cï¼Œæ­£ç¡®=C
    llm3 = ask_llm([medium_code, slow_code, fast_code], lang, prompt_type, is_triple=True)
    result.test3_llm_prediction = llm3["prediction"]
    result.test3_llm_raw_response = llm3["raw_response"]
    result.test3_reasoning_trace = llm3["reasoning_trace"]
    
    if llm3["prediction"] in ["A", "B", "C"]:
        result.test3_correct = (llm3["prediction"] == "C")
    elif llm3["prediction"] == "PARSE_FAILED":
        result.error = "test3_answer_parse_failed"
        return result
    else:
        result.error = f"test3_error"
        return result
    
    # ç»¼åˆåˆ†æ
    result.num_correct = sum([
        result.test1_correct or False,
        result.test2_correct or False,
        result.test3_correct or False
    ])
    result.is_all_correct = (result.num_correct == 3)
    
    t1_chose_fast = (result.test1_llm_prediction == "A")
    t2_chose_fast = (result.test2_llm_prediction == "B")
    t3_chose_fast = (result.test3_llm_prediction == "C")
    
    result.is_consistent = (t1_chose_fast == t2_chose_fast == t3_chose_fast)
    
    if result.is_all_correct:
        result.category = "all_correct"
    elif result.num_correct == 0:
        result.category = "all_wrong"
    elif result.is_consistent:
        result.category = "consistent_partial"
    else:
        result.category = "position_bias"
    
    return result


# ==================== è¯„ä¼°å™¨ ====================
class Evaluator:
    """ç»Ÿä¸€è¯„ä¼°å™¨"""
    
    def __init__(self, output_path: str, num_workers: int = 8, prompt_type: str = "zero-shot"):
        self.output_path = output_path
        self.num_workers = num_workers
        self.prompt_type = prompt_type
        self.processed_ids: set = set()
        self.pair_results: List[PairEvalResult] = []
        self.triple_results: List[TripleEvalResult] = []
        self.lock = Lock()
        
        self.stats = self._init_stats()
        self._load_processed()
    
    def _init_stats(self):
        return {
            "total": 0, "success": 0, "errors": 0, "parse_errors": 0,
            "pair_both_correct": 0, "pair_both_wrong": 0, 
            "pair_position_bias": 0,
            "triple_all_correct": 0, "triple_all_wrong": 0,
            "triple_partial_correct": 0, "triple_position_bias": 0,
            "consistent": 0, "inconsistent": 0,
            "by_dataset": {},
            "by_speedup_bin": {},  # æŒ‰ speedup_bin ç»Ÿè®¡
            "total_prompt_tokens": 0,
            "total_completion_tokens": 0,
            "total_tokens": 0,
            "total_cost_usd": 0.0  # æ€»æˆæœ¬ï¼ˆç¾å…ƒï¼‰
        }
    
    def _load_processed(self):
        """åŠ è½½å·²å¤„ç†çš„è®°å½•"""
        if Path(self.output_path).exists():
            try:
                with open(self.output_path, 'r') as f:
                    data = json.load(f)
                    
                    for r in data.get("pair_results", []):
                        self.processed_ids.add(r.get("pair_id", ""))
                        self.pair_results.append(PairEvalResult(**r))
                    
                    for r in data.get("triple_results", []):
                        self.processed_ids.add(r.get("pair_id", ""))
                        self.triple_results.append(TripleEvalResult(**r))
                    
                    self._recalculate_stats()
                    
                print(f"[INFO] å·²åŠ è½½ {len(self.processed_ids)} æ¡å†å²è®°å½•")
            except Exception as e:
                print(f"[WARN] åŠ è½½å†å²è®°å½•å¤±è´¥: {e}")
    
    def _recalculate_stats(self):
        """é‡æ–°è®¡ç®—ç»Ÿè®¡"""
        self.stats = self._init_stats()
        for r in self.pair_results:
            self._update_pair_stats(r, save=False)
        for r in self.triple_results:
            self._update_triple_stats(r, save=False)
    
    def _update_pair_stats(self, result: PairEvalResult, save: bool = True):
        with self.lock:
            self.stats["total"] += 1
            
            # æ±‡æ€» token ä½¿ç”¨ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬æ•°æ®ï¼‰
            test1_prompt = getattr(result, 'test1_prompt_tokens', 0)
            test1_completion = getattr(result, 'test1_completion_tokens', 0)
            test2_prompt = getattr(result, 'test2_prompt_tokens', 0)
            test2_completion = getattr(result, 'test2_completion_tokens', 0)
            
            self.stats["total_prompt_tokens"] += (test1_prompt + test2_prompt)
            self.stats["total_completion_tokens"] += (test1_completion + test2_completion)
            self.stats["total_tokens"] += (
                getattr(result, 'test1_total_tokens', 0) + 
                getattr(result, 'test2_total_tokens', 0)
            )
            
            # è®¡ç®—æˆæœ¬
            cost1 = calculate_cost(test1_prompt, test1_completion, MODEL)
            cost2 = calculate_cost(test2_prompt, test2_completion, MODEL)
            self.stats["total_cost_usd"] += (cost1 + cost2)
            
            ds_type = result.dataset_type
            if ds_type not in self.stats["by_dataset"]:
                self.stats["by_dataset"][ds_type] = {"total": 0, "correct": 0}
            self.stats["by_dataset"][ds_type]["total"] += 1
            
            # æŒ‰ speedup_bin ç»Ÿè®¡
            sbin = result.speedup_bin
            if sbin:
                if sbin not in self.stats["by_speedup_bin"]:
                    self.stats["by_speedup_bin"][sbin] = {
                        "total": 0, "correct": 0,
                        "both_correct": 0, "both_wrong": 0, "position_bias": 0,
                        "bin_idx": result.speedup_bin_idx
                    }
                self.stats["by_speedup_bin"][sbin]["total"] += 1
            
            if result.error:
                self.stats["errors"] += 1
                if "parse_failed" in str(result.error):
                    self.stats["parse_errors"] += 1
            else:
                self.stats["success"] += 1
                
                if result.category == "both_correct":
                    self.stats["pair_both_correct"] += 1
                    self.stats["by_dataset"][ds_type]["correct"] += 1
                    # æŒ‰ bin ç»Ÿè®¡
                    if sbin and sbin in self.stats["by_speedup_bin"]:
                        self.stats["by_speedup_bin"][sbin]["correct"] += 1
                        self.stats["by_speedup_bin"][sbin]["both_correct"] += 1
                elif result.category == "both_wrong":
                    self.stats["pair_both_wrong"] += 1
                    if sbin and sbin in self.stats["by_speedup_bin"]:
                        self.stats["by_speedup_bin"][sbin]["both_wrong"] += 1
                elif result.category == "position_bias":
                    self.stats["pair_position_bias"] += 1
                    if sbin and sbin in self.stats["by_speedup_bin"]:
                        self.stats["by_speedup_bin"][sbin]["position_bias"] += 1
                
                if result.is_consistent:
                    self.stats["consistent"] += 1
                else:
                    self.stats["inconsistent"] += 1
            
            if save:
                self.pair_results.append(result)
    
    def _update_triple_stats(self, result: TripleEvalResult, save: bool = True):
        with self.lock:
            self.stats["total"] += 1
            
            # æ±‡æ€» token ä½¿ç”¨ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬æ•°æ®ï¼‰
            test1_prompt = getattr(result, 'test1_prompt_tokens', 0)
            test1_completion = getattr(result, 'test1_completion_tokens', 0)
            test2_prompt = getattr(result, 'test2_prompt_tokens', 0)
            test2_completion = getattr(result, 'test2_completion_tokens', 0)
            test3_prompt = getattr(result, 'test3_prompt_tokens', 0)
            test3_completion = getattr(result, 'test3_completion_tokens', 0)
            
            self.stats["total_prompt_tokens"] += (test1_prompt + test2_prompt + test3_prompt)
            self.stats["total_completion_tokens"] += (test1_completion + test2_completion + test3_completion)
            self.stats["total_tokens"] += (
                getattr(result, 'test1_total_tokens', 0) + 
                getattr(result, 'test2_total_tokens', 0) + 
                getattr(result, 'test3_total_tokens', 0)
            )
            
            # è®¡ç®—æˆæœ¬
            cost1 = calculate_cost(test1_prompt, test1_completion, MODEL)
            cost2 = calculate_cost(test2_prompt, test2_completion, MODEL)
            cost3 = calculate_cost(test3_prompt, test3_completion, MODEL)
            self.stats["total_cost_usd"] += (cost1 + cost2 + cost3)
            
            ds_type = result.dataset_type
            if ds_type not in self.stats["by_dataset"]:
                self.stats["by_dataset"][ds_type] = {"total": 0, "correct": 0}
            self.stats["by_dataset"][ds_type]["total"] += 1
            
            if result.error:
                self.stats["errors"] += 1
                if "parse_failed" in str(result.error):
                    self.stats["parse_errors"] += 1
            else:
                self.stats["success"] += 1
                
                if result.category == "all_correct":
                    self.stats["triple_all_correct"] += 1
                    self.stats["by_dataset"][ds_type]["correct"] += 1
                elif result.category == "all_wrong":
                    self.stats["triple_all_wrong"] += 1
                elif result.category == "consistent_partial":
                    self.stats["triple_partial_correct"] += 1
                elif result.category == "position_bias":
                    self.stats["triple_position_bias"] += 1
                
                if result.is_consistent:
                    self.stats["consistent"] += 1
                else:
                    self.stats["inconsistent"] += 1
            
            if save:
                self.triple_results.append(result)
    
    def _calculate_accuracy(self) -> dict:
        """è®¡ç®—å„é¡¹å‡†ç¡®ç‡æŒ‡æ ‡"""
        s = self.stats
        success = s["success"]
        accuracy = {}
        
        if success == 0:
            return accuracy
        
        pair_correct = s["pair_both_correct"]
        triple_correct = s["triple_all_correct"]
        total_correct = pair_correct + triple_correct
        
        accuracy["overall_accuracy"] = round(total_correct / success * 100, 2)
        accuracy["overall_correct"] = total_correct
        accuracy["overall_total"] = success
        
        accuracy["consistency_rate"] = round(s["consistent"] / success * 100, 2)
        accuracy["consistent_count"] = s["consistent"]
        
        accuracy["by_dataset"] = {}
        for ds_type, ds_stats in s.get("by_dataset", {}).items():
            total = ds_stats["total"]
            if total > 0:
                correct = ds_stats["correct"]
                accuracy["by_dataset"][ds_type] = {
                    "accuracy": round(correct / total * 100, 2),
                    "correct": correct,
                    "total": total
                }
        
        pair_total = s['pair_both_correct'] + s['pair_both_wrong'] + s['pair_position_bias']
        if pair_total > 0:
            accuracy["pair"] = {
                "total": pair_total,
                "both_correct": s['pair_both_correct'],
                "both_correct_rate": round(s['pair_both_correct'] / pair_total * 100, 2),
                "both_wrong": s['pair_both_wrong'],
                "both_wrong_rate": round(s['pair_both_wrong'] / pair_total * 100, 2),
                "position_bias": s['pair_position_bias'],
                "position_bias_rate": round(s['pair_position_bias'] / pair_total * 100, 2)
            }
        
        triple_total = s['triple_all_correct'] + s['triple_all_wrong'] + s['triple_partial_correct'] + \
                       s['triple_position_bias']
        if triple_total > 0:
            accuracy["triple"] = {
                "total": triple_total,
                "all_correct": s['triple_all_correct'],
                "all_correct_rate": round(s['triple_all_correct'] / triple_total * 100, 2),
                "all_wrong": s['triple_all_wrong'],
                "all_wrong_rate": round(s['triple_all_wrong'] / triple_total * 100, 2),
                "partial_correct": s['triple_partial_correct'],
                "partial_correct_rate": round(s['triple_partial_correct'] / triple_total * 100, 2),
                "position_bias": s['triple_position_bias'],
                "position_bias_rate": round(s['triple_position_bias'] / triple_total * 100, 2)
            }
        
        # æŒ‰ speedup_bin ç»Ÿè®¡
        accuracy["by_speedup_bin"] = {}
        for bin_name, bin_stats in s.get("by_speedup_bin", {}).items():
            total = bin_stats["total"]
            if total > 0:
                correct = bin_stats["correct"]
                accuracy["by_speedup_bin"][bin_name] = {
                    "bin_idx": bin_stats.get("bin_idx", -1),
                    "total": total,
                    "correct": correct,
                    "accuracy": round(correct / total * 100, 2),
                    "both_correct": bin_stats["both_correct"],
                    "both_wrong": bin_stats["both_wrong"],
                    "position_bias": bin_stats["position_bias"]
                }
        
        return accuracy
    
    def _save_results(self):
        """ä¿å­˜ç»“æœ"""
        accuracy = self._calculate_accuracy()
        
        output_data = {
            "config": {
                "model": MODEL,
                "prompt_type": self.prompt_type,
                "version": "v8_full_experiment_design"
            },
            "accuracy": accuracy,
            "stats": self.stats,
            "pair_results": [asdict(r) for r in self.pair_results],
            "triple_results": [asdict(r) for r in self.triple_results]
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(self.output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
            
        with open(self.output_path, 'w') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    def evaluate(self, data_path: str, limit: int = 50):
        """ä¸»è¯„ä¼°æµç¨‹"""
        
        tasks = []
        is_triple_dataset = False
        dataset_types_found = set()
        
        with open(data_path, 'r') as f:
            for i, line in enumerate(f):
                if limit and len(tasks) >= limit:
                    break
                try:
                    data = json.loads(line)
                except json.JSONDecodeError:
                    continue
                
                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†: ä¼˜å…ˆç”¨ pair_id, å¦åˆ™ç”¨ problem_id+speedup, æœ€åç”¨è¡Œå·
                pair_id = data.get("pair_id")
                if not pair_id:
                    problem_id = data.get("problem_id", "")
                    speedup = data.get("speedup", "")
                    if problem_id:
                        pair_id = f"{problem_id}_{speedup}" if speedup else f"{problem_id}_line{i}"
                    else:
                        pair_id = f"line_{i}"
                # æ³¨å…¥åˆ° data ä¸­ä¾›åç»­å¤„ç†ä½¿ç”¨
                data["_generated_pair_id"] = pair_id
                
                if str(pair_id) in self.processed_ids:
                    continue
                
                dtype = detect_dataset_type(data)
                dataset_types_found.add(dtype)
                if dtype == "triple":
                    is_triple_dataset = True
                
                tasks.append(data)
        
        if not tasks:
            print("[INFO] æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–°æ•°æ®")
            self._print_stats(is_triple_dataset)
            return
        
        effective_workers = min(self.num_workers, 64)  # å¢åŠ ä¸Šé™è‡³ 64ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸ 
        
        print(f"[INFO] æ•°æ®é›†ç±»å‹: {', '.join(dataset_types_found)}")
        for dtype in dataset_types_found:
            if dtype in ("same_tier", "clean_vs_raw"):
                print(f"  - {dtype}: æ­£ç¡®ç­”æ¡ˆ = Similar")
            else:
                print(f"  - {dtype}: æ­£ç¡®ç­”æ¡ˆ = fast")
        
        print(f"[INFO] å¾…å¤„ç†: {len(tasks)} æ¡")
        print(f"[INFO] Promptç‰ˆæœ¬: v8 (å®Œæ•´å®éªŒè®¾è®¡)")
        print(f"[INFO] Promptç­–ç•¥: {self.prompt_type}")
        print(f"[INFO] å¹¶å‘æ•°: {effective_workers}")
        print("=" * 60)
        
        try:
            from tqdm import tqdm
            HAS_TQDM = True
        except ImportError:
            HAS_TQDM = False
        
        def process_item(data):
            dtype = detect_dataset_type(data)
            if dtype == "triple":
                return ("triple", process_triple(data, self.prompt_type))
            else:
                return ("pair", process_pair(data, self.prompt_type))
        
        completed = 0
        
        with ThreadPoolExecutor(max_workers=effective_workers) as executor:
            future_to_data = {executor.submit(process_item, d): d for d in tasks}
            
            iterator = tqdm(as_completed(future_to_data), total=len(tasks), 
                          desc="è¯„ä¼°ä¸­") if HAS_TQDM else as_completed(future_to_data)
            
            try:
                for future in iterator:
                    result_type, result = future.result()
                    
                    if result_type == "triple":
                        self._update_triple_stats(result)
                    else:
                        self._update_pair_stats(result)
                    
                    completed += 1
                    
                    if completed % 5 == 0:
                        self._save_results()
                        
            except KeyboardInterrupt:
                print(f"\n[WARN] ç”¨æˆ·ä¸­æ–­")
            finally:
                self._save_results()
        
        self._print_stats(is_triple_dataset)
    
    def _print_stats(self, is_triple: bool = False):
        """æ‰“å°ç»Ÿè®¡"""
        print("\n" + "=" * 70)
        print(f"è¯„ä¼°æŠ¥å‘Š - v8å®Œæ•´å®éªŒè®¾è®¡ç‰ˆ - {self.prompt_type.upper()}")
        print("=" * 70)
        
        s = self.stats
        success = s["success"]
        
        print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡")
        print(f"   æ€»æ•°: {s['total']}, æˆåŠŸ: {success}, é”™è¯¯: {s['errors']}")
        
        print(f"\nğŸ’° Token ä½¿ç”¨ç»Ÿè®¡")
        print(f"   Prompt Tokens: {s['total_prompt_tokens']:,}")
        print(f"   Completion Tokens: {s['total_completion_tokens']:,}")
        print(f"   Total Tokens: {s['total_tokens']:,}")
        if success > 0:
            print(f"   å¹³å‡æ¯æ ·æœ¬: {s['total_tokens'] / success:.0f} tokens")
        
        print(f"\nğŸ’µ æˆæœ¬ç»Ÿè®¡ (æ¨¡å‹: {MODEL})")
        pricing = get_model_pricing(MODEL)
        print(f"   å®šä»·: Input=${pricing['input']:.2f}/1M tokens, Output=${pricing['output']:.2f}/1M tokens")
        print(f"   æ€»æˆæœ¬: ${s['total_cost_usd']:.4f} USD")
        if success > 0:
            print(f"   å¹³å‡æ¯æ ·æœ¬: ${s['total_cost_usd'] / success:.6f} USD")
        
        if success == 0:
            return
        
        accuracy = self._calculate_accuracy()
        
        print(f"\nğŸ“Š æ€»ä½“å‡†ç¡®ç‡")
        print(f"   å‡†ç¡®ç‡: {accuracy.get('overall_accuracy', 0):.2f}%")
        print(f"   æ­£ç¡®: {accuracy.get('overall_correct', 0)} / {accuracy.get('overall_total', 0)}")
        
        print(f"\nğŸ“Š æŒ‰æ•°æ®é›†ç±»å‹ç»Ÿè®¡")
        for ds_type, ds_acc in accuracy.get("by_dataset", {}).items():
            print(f"   {ds_type}:")
            print(f"     å‡†ç¡®ç‡: {ds_acc['accuracy']:.2f}%")
            print(f"     æ­£ç¡®: {ds_acc['correct']} / {ds_acc['total']}")
        
        print(f"\nğŸ“Š ä¸€è‡´æ€§")
        print(f"   ä¸€è‡´æ€§ç‡: {accuracy.get('consistency_rate', 0):.2f}%")
        
        if "pair" in accuracy:
            p = accuracy["pair"]
            print(f"\nğŸ“Š åŒä»£ç é…å¯¹è¯¦ç»†åˆ†ç±»")
            print(f"   ä¸¤æ¬¡éƒ½å¯¹: {p['both_correct']} ({p['both_correct_rate']:.1f}%)")
            print(f"   ä¸¤æ¬¡éƒ½é”™: {p['both_wrong']} ({p['both_wrong_rate']:.1f}%)")
            print(f"   ä½ç½®åå·®: {p['position_bias']} ({p['position_bias_rate']:.1f}%)")
        
        if "triple" in accuracy:
            t = accuracy["triple"]
            print(f"\nğŸ“Š ä¸‰å…ƒç»„è¯¦ç»†åˆ†ç±»")
            print(f"   å…¨éƒ¨æ­£ç¡®: {t['all_correct']} ({t['all_correct_rate']:.1f}%)")
            print(f"   å…¨éƒ¨é”™è¯¯: {t['all_wrong']} ({t['all_wrong_rate']:.1f}%)")
            print(f"   éƒ¨åˆ†æ­£ç¡®: {t['partial_correct']} ({t['partial_correct_rate']:.1f}%)")
            print(f"   ä½ç½®åå·®: {t['position_bias']} ({t['position_bias_rate']:.1f}%)")
        
        # æŒ‰ Speedup Bin ç»Ÿè®¡
        if accuracy.get("by_speedup_bin"):
            print(f"\nğŸ“Š æŒ‰ Speedup åˆ†æ¡£å‡†ç¡®ç‡")
            print(f"   {'Bin':<18} {'æ ·æœ¬':>6} {'æ­£ç¡®':>6} {'å‡†ç¡®ç‡':>8}")
            print(f"   {'-'*40}")
            # æŒ‰ bin_idx æ’åº
            sorted_bins = sorted(
                accuracy["by_speedup_bin"].items(),
                key=lambda x: x[1].get("bin_idx", 999)
            )
            for bin_name, bin_acc in sorted_bins:
                print(f"   {bin_name:<18} {bin_acc['total']:>6} {bin_acc['correct']:>6} {bin_acc['accuracy']:>7.1f}%")
        
        print(f"\nç»“æœå·²ä¿å­˜: {self.output_path}")


# ==================== å…¥å£ ====================
def main():
    global MODEL, API_KEY, BASE_URL, client
    
    parser = argparse.ArgumentParser(
        description="LLM ä»£ç æ€§èƒ½åˆ¤æ–­è¯„ä¼° v9 - ç®€åŒ–ä¸º3ç§Promptç­–ç•¥",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
v9 ç®€åŒ–è®¾è®¡:
  æ”¯æŒ 3 ç§ Prompt ç­–ç•¥ï¼Œæ‰€æœ‰ç¤ºä¾‹å‡ä¸ºè¯­è¨€ç‰¹å®š:
  
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Promptç­–ç•¥       â”‚ ç¤ºä¾‹   â”‚ CoTæ¨ç† â”‚ è¯­è¨€ç‰¹å®šç¤ºä¾‹ â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ zero-shot (ZS)   â”‚   âŒ   â”‚   âŒ    â”‚      -       â”‚
  â”‚ few-shot (FS)    â”‚   âœ…   â”‚   âŒ    â”‚   âœ…(åŒ¹é…)   â”‚
  â”‚ few-shot-cot     â”‚   âœ…   â”‚   âœ…    â”‚   âœ…(åŒ¹é…)   â”‚
  â”‚ (FS-CoT)         â”‚        â”‚         â”‚              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç¤ºä¾‹:
  # 1. Zero-Shot (ZS): ç›´æ¥æ¯”è¾ƒï¼Œæ— ç¤ºä¾‹ï¼Œæ— æ¨ç†
  python eval_full.py data.jsonl --prompt-type zero-shot
  
  # 2. Few-Shot (FS): æœ‰è¯­è¨€ç‰¹å®šç¤ºä¾‹ï¼Œåªå±•ç¤ºè¾“å…¥è¾“å‡º
  python eval_full.py data.jsonl --prompt-type few-shot
  
  # 3. Few-Shot CoT (FS-CoT): æœ‰è¯­è¨€ç‰¹å®šç¤ºä¾‹ï¼Œå±•ç¤ºè¯¦ç»†æ¨ç†è¿‡ç¨‹
  python eval_full.py data.jsonl --prompt-type few-shot-cot

æ‰¹é‡è¿è¡Œæ‰€æœ‰ç­–ç•¥:
  for pt in zero-shot few-shot few-shot-cot; do
    python eval_full.py data.jsonl --prompt-type $pt -n 100
  done
        """
    )
    
    parser.add_argument("data", help="è¾“å…¥ JSONL æ•°æ®è·¯å¾„")
    parser.add_argument("-o", "--output", default=None, 
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (è‹¥ä¸æŒ‡å®šï¼Œå°†è‡ªåŠ¨æ ¹æ®è¾“å…¥æ–‡ä»¶åã€promptç±»å‹å’Œæ¨¡å‹åç§°ç”Ÿæˆ)")
    parser.add_argument("-n", "--limit", type=int, default=5)
    parser.add_argument("-j", "--workers", type=int, default=8)
    parser.add_argument("--prompt-type", 
                       choices=["zero-shot", "few-shot", "few-shot-cot"], 
                       default="zero-shot",
                       help="""Promptç­–ç•¥é€‰æ‹©:
  zero-shot    : Zero-Shot (ZS) - ç›´æ¥æ¯”è¾ƒï¼Œæ— ç¤ºä¾‹ï¼Œæ— æ¨ç†
  few-shot     : Few-Shot (FS) - æœ‰2ä¸ªè¯­è¨€ç‰¹å®šç¤ºä¾‹ï¼Œåªå±•ç¤ºè¾“å…¥è¾“å‡º
  few-shot-cot : Few-Shot CoT (FS-CoT) - æœ‰2ä¸ªè¯­è¨€ç‰¹å®šç¤ºä¾‹ï¼Œå±•ç¤ºè¯¦ç»†æ¨ç†è¿‡ç¨‹""")
    parser.add_argument("--model", default=None)
    parser.add_argument("--api-key", default=None)
    parser.add_argument("--base-url", default=None)
    
    args = parser.parse_args()
    
    if args.model:
        MODEL = args.model
    if args.api_key:
        API_KEY = args.api_key
    if args.base_url:
        BASE_URL = args.base_url
    
    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    # å¦‚æœç”¨æˆ·æŒ‡å®šäº† -o/--outputï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„è·¯å¾„
    # å¦åˆ™è‡ªåŠ¨æ ¹æ®è¾“å…¥æ–‡ä»¶åã€promptç±»å‹å’Œæ¨¡å‹åç§°ç”Ÿæˆ
    if args.output:
        output_path = args.output
    else:
        output_path = generate_default_output_filename(args.data, args.prompt_type, MODEL)
    
    print(f"LLM ä»£ç æ€§èƒ½è¯„ä¼° v9 (ç®€åŒ–ä¸º3ç§Promptç­–ç•¥)")
    print(f"{'='*60}")
    print(f"è¾“å…¥: {args.data}")
    print(f"è¾“å‡º: {output_path}")
    print(f"æ¨¡å‹: {MODEL}")
    print(f"Prompt: {args.prompt_type}")
    print(f"{'='*60}\n")
    
    evaluator = Evaluator(output_path, args.workers, args.prompt_type)
    evaluator.evaluate(args.data, args.limit if args.limit > 0 else None)


if __name__ == "__main__":
    main()